"""Probability-aware phase runner for two-phase GBM training with probability targets."""

import os
import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import gc
import time
from typing import Dict, Any, Optional, List

# Import our modules
from GenerativeBrainModel.models.gbm import GBM
from GenerativeBrainModel.datasets.probability_data_loader import SubjectFilteredProbabilityDALIBrainDataLoader
from GenerativeBrainModel.training.memory_utils import print_memory_stats
from GenerativeBrainModel.training.schedulers import get_lr_scheduler
from GenerativeBrainModel.evaluation.metrics import track_metrics_during_validation
from GenerativeBrainModel.evaluation.data_saver import save_test_data_and_predictions
from GenerativeBrainModel.custom_functions.visualization import create_prediction_video
from GenerativeBrainModel.utils.file_utils import create_experiment_dir, save_losses_to_csv
from GenerativeBrainModel.utils.data_utils import get_max_z_planes
from .phase_runner import TwoPhaseTrainer


class ProbabilityTwoPhaseTrainer(TwoPhaseTrainer):
    """Orchestrates two-phase GBM training with probability targets: pretrain on multiple subjects, then finetune on target subject."""
    
    def __init__(
        self,
        exp_root: str,
        pretrain_params: Dict[str, Any],
        finetune_params: Dict[str, Any],
        target_subject: str,
        skip_pretrain: bool = False,
        pretrain_checkpoint: Optional[str] = None,
        use_probabilities: bool = True
    ):
        """Initialize the probability-aware two-phase trainer.
        
        Args:
            exp_root: Root experiment directory
            pretrain_params: Parameters for pretraining phase
            finetune_params: Parameters for finetuning phase
            target_subject: Name of target subject for finetuning
            skip_pretrain: Whether to skip pretraining phase
            pretrain_checkpoint: Path to existing pretrain checkpoint
            use_probabilities: If True, use probability grids as targets. If False, use binary spikes.
        """
        # Initialize parent class
        super().__init__(
            exp_root=exp_root,
            pretrain_params=pretrain_params,
            finetune_params=finetune_params,
            target_subject=target_subject,
            skip_pretrain=skip_pretrain,
            pretrain_checkpoint=pretrain_checkpoint
        )
        
        # Store probability-specific parameters
        self.use_probabilities = use_probabilities
        
        # Add probability info to both phase parameters
        self.pretrain_params['use_probabilities'] = use_probabilities
        self.finetune_params['use_probabilities'] = use_probabilities
        
        tqdm.write(f"Probability Training Mode: {'ENABLED' if use_probabilities else 'DISABLED (using binary spikes)'}")
        
    def _create_data_loader(self, params, subjects_include, subjects_exclude, split, shuffle):
        """Create a subject-filtered probability data loader."""
        return SubjectFilteredProbabilityDALIBrainDataLoader(
            params['preaugmented_dir'],
            include_subjects=subjects_include,
            exclude_subjects=subjects_exclude,
            batch_size=params['batch_size'],
            seq_len=params['seq_len'],
            split=split,
            device_id=0,
            num_threads=params['dali_num_threads'],
            gpu_prefetch=params['gpu_prefetch'],
            seed=42 if split == 'train' else 43,
            shuffle=shuffle,
            stride=params['seq_stride'],
            use_probabilities=self.use_probabilities
        )
    
    def _save_model_info(self, model, params, phase_dir, phase_name):
        """Save model architecture and parameters to file."""
        with open(os.path.join(phase_dir, "model_architecture.txt"), "w") as f:
            f.write(f"{phase_name.upper()} Phase Model Architecture (Probability Targets):\n")
            f.write("=" * 50 + "\n\n")
            f.write(str(model))
            f.write("\n\n" + "=" * 50 + "\n\n")
            f.write("Model Parameters:\n")
            for key, value in params.items():
                f.write(f"{key}: {value}\n")
            
            # Add probability-specific information
            f.write(f"\nProbability Training Configuration:\n")
            f.write(f"use_probabilities: {self.use_probabilities}\n")
            if self.use_probabilities:
                f.write(f"Target type: Continuous probability values (0.0 to ~30.0)\n")
                f.write(f"Loss function: BCE with logits (supports continuous targets)\n")
            else:
                f.write(f"Target type: Binary spike values (0.0 or 1.0)\n")
                f.write(f"Loss function: BCE with logits (binary targets)\n")
            
            # Add statistics
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            f.write(f"\nModel Statistics:\n")
            f.write(f"Total parameters: {total_params:,}\n")
            f.write(f"Trainable parameters: {trainable_params:,}\n")
    
    def _run_training_loop(self, model, optimizer, lr_scheduler, train_loader, test_loader,
                          phase_dir, phase_name, params):
        """Run the main training loop for a phase with probability targets."""
        # Training state
        train_losses = []
        test_losses = []
        raw_batch_losses = []
        best_test_loss = float('inf')
        
        # Validation tracking
        validation_step_indices = []
        validation_losses = []
        validation_f1_scores = []
        validation_recall_scores = []
        validation_precision_scores = []
        
        # Test metrics (end of epoch)
        test_f1_scores = []
        test_recall_scores = []
        test_precision_scores = []
        
        # Training setup
        scaler = GradScaler()
        validation_interval = len(train_loader) // params['validation_per_epoch']
        quarter_epoch_size = len(train_loader) // 4
        best_model_checkpoint_path = os.path.join(phase_dir, 'checkpoints', 'best_model.pt')
        
        target_type = "probability" if self.use_probabilities else "binary"
        tqdm.write(f"Starting {phase_name} training with {target_type} targets...")
        tqdm.write(f"Validating every {validation_interval} batches ({params['validation_per_epoch']} times per epoch)")
        
        # Main training loop
        for epoch in range(params['num_epochs']):
            model.train()
            epoch_loss = 0.0
            batch_count = 0
            
            train_loader.reset()
            train_iter = iter(train_loader)
            
            train_loop = tqdm(range(len(train_loader)), 
                            desc=f"{phase_name.capitalize()} Epoch {epoch+1}/{params['num_epochs']} ({target_type})")
            
            for batch_idx in train_loop:
                try:
                    # Get batch
                    batch = next(train_iter)
                    if batch.device.type != 'cuda':
                        batch = batch.cuda(non_blocking=True)
                    
                    # Forward pass
                    optimizer.zero_grad()
                    
                    with autocast():
                        predictions = model(batch)
                        loss = model.compute_loss(predictions, batch[:, 1:])
                    
                    # Backward pass
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    
                    # Update learning rate
                    lr_scheduler.step()
                    
                    # Track metrics
                    current_loss = loss.item()
                    epoch_loss += current_loss
                    batch_count += 1
                    raw_batch_losses.append(current_loss)
                    
                    # Update progress bar
                    train_loop.set_postfix(loss=current_loss)
                    
                    # Clean up
                    del loss, predictions, batch
                    
                    # Validation
                    if (batch_idx + 1) % validation_interval == 0 or batch_idx == len(train_loader) - 1:
                        global_step = epoch * len(train_loader) + batch_idx
                        val_metrics = track_metrics_during_validation(model, test_loader, 'cuda')
                        
                        # Record validation metrics
                        validation_step_indices.append(global_step)
                        validation_losses.append(val_metrics['loss'])
                        validation_f1_scores.append(val_metrics['f1'])
                        validation_recall_scores.append(val_metrics['recall'])
                        validation_precision_scores.append(val_metrics['precision'])
                        
                        tqdm.write(f"Validation at step {global_step}: Loss={val_metrics['loss']:.6f}, "
                                  f"F1={val_metrics['f1']:.6f}, Recall={val_metrics['recall']:.6f}, "
                                  f"Precision={val_metrics['precision']:.6f}")
                        
                        # Save best model
                        if val_metrics['loss'] < best_test_loss:
                            best_test_loss = val_metrics['loss']
                            self._save_checkpoint(model, optimizer, lr_scheduler, epoch+1,
                                               train_losses, test_losses, validation_losses,
                                               validation_step_indices, raw_batch_losses,
                                               params, best_test_loss, phase_name,
                                               best_model_checkpoint_path)
                        
                        # Switch back to training mode
                        model.train()
                    
                    # Generate comparison video every quarter epoch
                    if (batch_idx + 1) % quarter_epoch_size == 0:
                        quarter = (batch_idx + 1) // quarter_epoch_size
                        video_path = os.path.join(phase_dir, 'videos', 
                                                f'predictions_epoch_{epoch+1:03d}_quarter_{quarter}.mp4')
                        create_prediction_video(model, test_loader, video_path, num_frames=330)
                    
                    # Memory cleanup
                    if batch_idx % 50 == 0:
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                except StopIteration:
                    break
            
            # End of epoch evaluation
            avg_train_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')
            train_losses.append(avg_train_loss)
            
            # Test evaluation
            test_metrics = track_metrics_during_validation(model, test_loader, 'cuda')
            test_losses.append(test_metrics['loss'])
            test_f1_scores.append(test_metrics['f1'])
            test_recall_scores.append(test_metrics['recall'])
            test_precision_scores.append(test_metrics['precision'])
            
            tqdm.write(f"{phase_name.capitalize()} Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f}, "
                      f"Test Loss = {test_metrics['loss']:.6f}, Test F1 = {test_metrics['f1']:.6f}")
            
            # Create epoch video
            video_path = os.path.join(phase_dir, 'videos', f'predictions_epoch_{epoch+1:03d}_final.mp4')
            create_prediction_video(model, test_loader, video_path, num_frames=330)
            
            # Update plots and save data
            self._update_plots_and_save_data(
                phase_dir, phase_name, train_losses, test_losses, raw_batch_losses,
                validation_step_indices, validation_losses, validation_f1_scores,
                validation_recall_scores, validation_precision_scores,
                test_f1_scores, test_recall_scores, test_precision_scores,
                len(train_loader)
            )
        
        # Save final model
        final_model_path = os.path.join(phase_dir, 'checkpoints', 'final_model.pt')
        self._save_checkpoint(model, optimizer, lr_scheduler, params['num_epochs'],
                           train_losses, test_losses, validation_losses,
                           validation_step_indices, raw_batch_losses, params,
                           avg_train_loss, phase_name, final_model_path)
        
        # Create final prediction video
        video_path = os.path.join(phase_dir, 'videos', 'final_predictions.mp4')
        create_prediction_video(model, test_loader, video_path, num_frames=330)
        
        tqdm.write(f"{phase_name.capitalize()} training complete!")
        
        return best_model_checkpoint_path 