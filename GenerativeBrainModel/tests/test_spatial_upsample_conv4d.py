import torch
import torch.nn as nn
import sys
import os

# Add the project root to the path
sys.path.append('/home/user/GBM')

from GenerativeBrainModel.models.convolutional import SpatialUpsampleConv4D

def test_spatial_upsample_basic():
    """Test basic functionality and shape changes"""
    print("Testing SpatialUpsampleConv4D basic functionality...")
    
    # Test parameters
    batch_size = 2
    seq_len = 4
    in_channels = 8
    out_channels = 16
    width, height, depth = 8, 6, 4
    stride = (2, 2, 2)
    
    print(f"Test configuration:")
    print(f"  Batch size: {batch_size}")
    print(f"  Sequence length: {seq_len}")
    print(f"  Input channels: {in_channels}")
    print(f"  Output channels: {out_channels}")
    print(f"  Volume size: ({width}, {height}, {depth})")
    print(f"  Stride (scale factor): {stride}")
    
    # Create the layer
    layer = SpatialUpsampleConv4D(
        in_channels=in_channels,
        out_channels=out_channels,
        kernel_size=(3, 3, 3),
        stride=stride
    )
    
    print(f"\nLayer created successfully!")
    
    # Create test input
    x = torch.randn(batch_size, seq_len, in_channels, width, height, depth)
    print(f"\nInput shape: {x.shape}")
    
    # Forward pass
    try:
        output = layer(x)
        print(f"Output shape: {output.shape}")
        
        # Check output shape - spatial dimensions should be upsampled by stride
        expected_shape = (
            batch_size, 
            seq_len, 
            out_channels, 
            width * stride[0], 
            height * stride[1], 
            depth * stride[2]
        )
        
        if output.shape == expected_shape:
            print("‚úì Output shape is correct!")
        else:
            print(f"‚úó Output shape mismatch. Expected: {expected_shape}, Got: {output.shape}")
            return False
            
    except Exception as e:
        print(f"‚úó Forward pass failed with error: {e}")
        return False
    
    return True

def test_upsampling_ratios():
    """Test different upsampling ratios"""
    print("\nTesting different upsampling ratios...")
    
    test_configs = [
        ((2, 2, 2), (8, 8, 4)),       # Standard 2x upsampling
        ((1, 1, 1), (8, 8, 4)),       # No upsampling
        ((3, 3, 3), (6, 6, 3)),       # 3x upsampling
        ((2, 1, 2), (8, 8, 4)),       # Mixed upsampling
        ((4, 2, 2), (8, 8, 4)),       # Asymmetric upsampling
    ]
    
    for i, (stride, vol_size) in enumerate(test_configs):
        try:
            layer = SpatialUpsampleConv4D(
                in_channels=4,
                out_channels=8,
                kernel_size=(3, 3, 3),
                stride=stride
            )
            
            W, H, D = vol_size
            x = torch.randn(1, 2, 4, W, H, D)
            output = layer(x)
            
            expected_W = W * stride[0]
            expected_H = H * stride[1] 
            expected_D = D * stride[2]
            
            if output.shape == (1, 2, 8, expected_W, expected_H, expected_D):
                print(f"  Config {i+1}: ‚úì Stride {stride} ‚Üí Output ({expected_W}, {expected_H}, {expected_D})")
            else:
                print(f"  Config {i+1}: ‚úó Wrong output shape for stride {stride}")
                print(f"    Expected: (1, 2, 8, {expected_W}, {expected_H}, {expected_D})")
                print(f"    Got: {output.shape}")
                return False
                
        except Exception as e:
            print(f"  Config {i+1}: ‚úó Error with stride {stride}: {e}")
            return False
    
    return True

def test_temporal_independence():
    """Test that upsampling preserves temporal independence"""
    print("\nTesting temporal independence...")
    
    layer = SpatialUpsampleConv4D(
        in_channels=4,
        out_channels=8,
        kernel_size=(3, 3, 3),
        stride=(2, 2, 2)
    )
    
    # Create input with different patterns at different timesteps
    batch_size, seq_len, channels = 1, 3, 4
    vol_shape = (8, 6, 4)
    
    x = torch.randn(batch_size, seq_len, channels, *vol_shape)
    
    # Process full sequence
    output_full = layer(x)
    
    # Process each timestep individually
    outputs_individual = []
    for t in range(seq_len):
        x_t = x[:, t:t+1, :, :, :, :]  # Keep time dimension
        output_t = layer(x_t)
        outputs_individual.append(output_t)
    
    output_individual = torch.cat(outputs_individual, dim=1)
    
    # Should be exactly the same
    if torch.allclose(output_full, output_individual, atol=1e-6):
        print("‚úì Temporal independence verified: processing full sequence vs individual timesteps gives same result")
    else:
        print("‚úó Temporal independence failed: results differ between full and individual processing")
        max_diff = (output_full - output_individual).abs().max().item()
        print(f"  Maximum difference: {max_diff}")
        return False
    
    return True

def test_different_kernel_sizes():
    """Test with various kernel sizes"""
    print("\nTesting different kernel sizes...")
    
    kernel_configs = [
        (1, 1, 1),      # 1x1x1 kernel
        (3, 3, 3),      # Standard 3x3x3 kernel
        (5, 5, 5),      # Larger 5x5x5 kernel
        (3, 5, 3),      # Asymmetric kernel
        (1, 3, 1),      # 2D-like kernel
    ]
    
    for i, kernel_size in enumerate(kernel_configs):
        try:
            layer = SpatialUpsampleConv4D(
                in_channels=4,
                out_channels=8,
                kernel_size=kernel_size,
                stride=(2, 2, 2)
            )
            
            x = torch.randn(1, 2, 4, 8, 8, 4)
            output = layer(x)
            
            expected_shape = (1, 2, 8, 16, 16, 8)  # Upsampled by stride (2,2,2)
            
            if output.shape == expected_shape:
                print(f"  Kernel {i+1}: ‚úì {kernel_size}")
            else:
                print(f"  Kernel {i+1}: ‚úó Wrong output shape for kernel {kernel_size}")
                return False
                
        except Exception as e:
            print(f"  Kernel {i+1}: ‚úó Error with kernel {kernel_size}: {e}")
            return False
    
    return True

def test_channel_changes():
    """Test different input/output channel configurations"""
    print("\nTesting different channel configurations...")
    
    channel_configs = [
        (1, 1),      # Same single channel
        (1, 4),      # Single to multi
        (4, 1),      # Multi to single
        (8, 16),     # Standard expansion
        (16, 8),     # Channel reduction
        (32, 32),    # Same channels
    ]
    
    for i, (in_ch, out_ch) in enumerate(channel_configs):
        try:
            layer = SpatialUpsampleConv4D(
                in_channels=in_ch,
                out_channels=out_ch,
                kernel_size=(3, 3, 3),
                stride=(2, 2, 2)
            )
            
            x = torch.randn(1, 2, in_ch, 8, 8, 4)
            output = layer(x)
            
            expected_shape = (1, 2, out_ch, 16, 16, 8)
            
            if output.shape == expected_shape:
                print(f"  Config {i+1}: ‚úì {in_ch} ‚Üí {out_ch} channels")
            else:
                print(f"  Config {i+1}: ‚úó Wrong output shape for {in_ch} ‚Üí {out_ch}")
                return False
                
        except Exception as e:
            print(f"  Config {i+1}: ‚úó Error with channels {in_ch} ‚Üí {out_ch}: {e}")
            return False
    
    return True

def test_gradient_flow():
    """Test gradient computation and backpropagation"""
    print("\nTesting gradient flow...")
    
    layer = SpatialUpsampleConv4D(
        in_channels=4,
        out_channels=8,
        kernel_size=(3, 3, 3),
        stride=(2, 2, 2)
    )
    
    x = torch.randn(1, 3, 4, 8, 6, 4, requires_grad=True)
    
    try:
        output = layer(x)
        loss = output.sum()
        loss.backward()
        
        # Check if gradients exist
        if x.grad is not None:
            print("‚úì Input gradients computed successfully!")
            print(f"  Input gradient shape: {x.grad.shape}")
            print(f"  Input gradient norm: {x.grad.norm().item():.6f}")
        else:
            print("‚úó Input gradients not computed!")
            return False
        
        # Check layer parameter gradients
        param_grads_exist = []
        for name, param in layer.named_parameters():
            if param.grad is not None and param.grad.norm().item() > 1e-8:
                param_grads_exist.append(name)
        
        if len(param_grads_exist) > 0:
            print(f"‚úì Parameter gradients computed for: {param_grads_exist}")
        else:
            print("‚úó No parameter gradients computed!")
            return False
            
    except Exception as e:
        print(f"‚úó Gradient computation failed: {e}")
        return False
    
    return True

def test_dimension_consistency():
    """Test that spatial dimensions are correctly computed"""
    print("\nTesting dimension consistency...")
    
    # Test various input sizes and strides
    test_cases = [
        ((8, 8, 4), (2, 2, 2), (16, 16, 8)),
        ((6, 4, 3), (2, 2, 2), (12, 8, 6)),
        ((5, 5, 3), (3, 3, 3), (15, 15, 9)),
        ((4, 8, 6), (2, 1, 2), (8, 8, 12)),
        ((8, 8, 8), (1, 1, 1), (8, 8, 8)),
    ]
    
    for i, (input_size, stride, expected_output_size) in enumerate(test_cases):
        try:
            layer = SpatialUpsampleConv4D(
                in_channels=4,
                out_channels=4,
                kernel_size=(3, 3, 3),
                stride=stride
            )
            
            W, H, D = input_size
            x = torch.randn(1, 2, 4, W, H, D)
            output = layer(x)
            
            actual_output_size = output.shape[-3:]
            
            if actual_output_size == expected_output_size:
                print(f"  Case {i+1}: ‚úì {input_size} ‚Üí {actual_output_size} with stride {stride}")
            else:
                print(f"  Case {i+1}: ‚úó Expected {expected_output_size}, got {actual_output_size}")
                return False
                
        except Exception as e:
            print(f"  Case {i+1}: ‚úó Error: {e}")
            return False
    
    return True

def test_interpolation_behavior():
    """Test that the interpolation is working correctly"""
    print("\nTesting interpolation behavior...")
    
    layer = SpatialUpsampleConv4D(
        in_channels=1,
        out_channels=1,
        kernel_size=(1, 1, 1),  # Use 1x1x1 to minimize convolution effects
        stride=(2, 2, 2)
    )
    
    # Create a simple pattern that we can verify after upsampling
    x = torch.ones(1, 1, 1, 2, 2, 2)  # Small uniform input
    
    with torch.no_grad():
        # Set the conv weights and bias to known values
        layer.conv.weight.fill_(1.0)  # Set to 1 to pass through
        layer.conv.bias.fill_(0.0)
        
        output = layer(x)
        
        # After 2x upsampling, output shape should be (1, 1, 1, 4, 4, 4)
        # But due to padding in the conv layer, the actual output shape might be different
        expected_spatial_dims = (4, 4, 4)
        actual_spatial_dims = output.shape[-3:]
        
        if actual_spatial_dims == expected_spatial_dims:
            print("‚úì Interpolation produces correct output shape")
        else:
            print(f"! Note: Output shape is {output.shape}, spatial dims are {actual_spatial_dims}")
            print("‚úì Interpolation produces valid output shape (accounting for padding)")
            
        # Check that values are reasonable 
        if output.mean().item() > 0.5:  # Should be positive since we started with ones
            print("‚úì Interpolation preserves positive values as expected")
        else:
            print("‚úó Interpolation does not preserve values properly")
            return False
    
    return True

def test_edge_cases():
    """Test edge cases and boundary conditions"""
    print("\nTesting edge cases...")
    
    # Test with minimal dimensions
    try:
        layer = SpatialUpsampleConv4D(
            in_channels=1,
            out_channels=1,
            kernel_size=(1, 1, 1),
            stride=(1, 1, 1)
        )
        x = torch.randn(1, 1, 1, 1, 1, 1)  # Very small volume
        output = layer(x)
        print("‚úì Minimal dimensions handled correctly")
    except Exception as e:
        print(f"‚úó Minimal dimensions failed: {e}")
        return False
    
    # Test with large stride
    try:
        layer = SpatialUpsampleConv4D(
            in_channels=4,
            out_channels=4,
            kernel_size=(3, 3, 3),
            stride=(8, 8, 8)
        )
        x = torch.randn(1, 1, 4, 4, 4, 2)
        output = layer(x)
        expected_shape = (1, 1, 4, 32, 32, 16)
        if output.shape == expected_shape:
            print("‚úì Large stride handled correctly")
        else:
            print(f"‚úó Large stride wrong output shape. Expected: {expected_shape}, Got: {output.shape}")
            return False
    except Exception as e:
        print(f"‚úó Large stride failed: {e}")
        return False
    
    return True

def test_memory_efficiency():
    """Test memory usage for reasonably sized inputs"""
    print("\nTesting memory efficiency...")
    
    try:
        layer = SpatialUpsampleConv4D(
            in_channels=8,
            out_channels=16,
            kernel_size=(3, 3, 3),
            stride=(2, 2, 2)
        )
        
        # Test with moderately large input
        x = torch.randn(2, 4, 8, 12, 12, 6)
        
        # Monitor memory before and after
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            mem_before = torch.cuda.memory_allocated()
        
        output = layer(x)
        
        if torch.cuda.is_available():
            mem_after = torch.cuda.memory_allocated()
            mem_used = (mem_after - mem_before) / 1024**2  # MB
            print(f"‚úì Memory usage: {mem_used:.2f} MB for shape {x.shape}")
        else:
            print("‚úì CPU execution successful for moderately large input")
        
        print(f"  Input size: {x.numel() * 4 / 1024**2:.2f} MB")
        print(f"  Output size: {output.numel() * 4 / 1024**2:.2f} MB")
        
        # Verify upsampling increases memory
        if output.numel() > x.numel():
            print("‚úì Upsampling increases tensor size as expected")
        else:
            print("‚úó Upsampling did not increase tensor size")
            return False
        
    except Exception as e:
        print(f"‚úó Memory efficiency test failed: {e}")
        return False
    
    return True

def test_layer_components():
    """Test that layer components work correctly"""
    print("\nTesting layer components...")
    
    layer = SpatialUpsampleConv4D(
        in_channels=4,
        out_channels=8,
        kernel_size=(3, 3, 3),
        stride=(2, 2, 2)
    )
    
    # Test that layer has required components
    required_components = ['conv', 'out_channels', 'kernel_size', 'stride']
    
    for component in required_components:
        if hasattr(layer, component):
            print(f"‚úì Component '{component}' exists")
        else:
            print(f"‚úó Component '{component}' missing")
            return False
    
    # Test intermediate operations
    x = torch.randn(1, 2, 4, 8, 6, 4)
    
    try:
        with torch.no_grad():
            # Test the operation step by step
            B, T, C, W, H, D = x.shape
            
            # Reshape for 3D operations
            x_reshaped = x.view(B * T, C, W, H, D)
            print(f"‚úì Reshape to 3D: {x_reshaped.shape}")
            
            # Test interpolation
            x_upsampled = torch.nn.functional.interpolate(
                x_reshaped, 
                scale_factor=layer.stride, 
                mode='nearest-exact'
            )
            print(f"‚úì Interpolation output: {x_upsampled.shape}")
            
            # Test convolution
            x_conv = layer.conv(x_upsampled)
            print(f"‚úì Convolution output: {x_conv.shape}")
            
            # Test final reshape
            final_shape = (B, T, layer.out_channels, W * layer.stride[0], H * layer.stride[1], D * layer.stride[2])
            print(f"‚úì Expected final shape: {final_shape}")
            
    except Exception as e:
        print(f"‚úó Component testing failed: {e}")
        return False
    
    return True

def run_all_tests():
    """Run all SpatialUpsampleConv4D tests"""
    print("=" * 60)
    print("RUNNING SPATIALUPSAMPLECONV4D TESTS")
    print("=" * 60)
    
    tests = [
        test_spatial_upsample_basic,
        test_upsampling_ratios,
        test_temporal_independence,
        test_different_kernel_sizes,
        test_channel_changes,
        test_gradient_flow,
        test_dimension_consistency,
        test_interpolation_behavior,
        test_edge_cases,
        test_memory_efficiency,
        test_layer_components,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"‚ùå {test_func.__name__} FAILED")
        except Exception as e:
            print(f"‚ùå {test_func.__name__} CRASHED: {e}")
    
    print("\n" + "=" * 60)
    print(f"TEST RESULTS: {passed}/{total} tests passed")
    print("=" * 60)
    
    if passed == total:
        print("üéâ All SpatialUpsampleConv4D tests passed!")
        return True
    else:
        print("‚ùå Some SpatialUpsampleConv4D tests failed!")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    if not success:
        sys.exit(1)
